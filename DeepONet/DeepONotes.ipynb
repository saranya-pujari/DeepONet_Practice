{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.gaussian_process'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgaussian_process\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GaussianProcessRegressor\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgaussian_process\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RBF\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.gaussian_process'"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from tqdm import tqdm\n",
    "from scipy.integrate import solve_ivp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(mean, var, verbose=False):\n",
    "    \"\"\"Definition of a DeepONet with fully connected branch and trunk layers.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    mean: dictionary, mean values of the inputs\n",
    "    var: dictionary, variance values of the inputs\n",
    "    verbose: boolean, indicate whether to show the model summary\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    model: the DeepONet model\n",
    "    \"\"\"\n",
    "    # Branch net\n",
    "    branch_input = tf.keras.Input(shape=(len(mean['forcing']),), name=\"forcing\")\n",
    "    branch = tf.keras.layers.Normalization(mean=mean['forcing'], variance=var['forcing'])(branch_input)\n",
    "    for _ in range(3):\n",
    "        branch = tf.keras.layers.Dense(50, activation=\"tanh\")(branch)\n",
    "\n",
    "    # Trunk net\n",
    "    trunk_input = tf.keras.Input(shape=(len(mean['time']),), name=\"time\")\n",
    "    trunk = tf.keras.layers.Normalization(mean=mean['time'], variance=var['time'])(trunk_input)\n",
    "    for _ in range(3):\n",
    "        trunk = tf.keras.layers.Dense(50, activation=\"tanh\")(trunk)\n",
    "\n",
    "    # Compute the dot product between branch and trunk net\n",
    "    dot_product = tf.keras.layers.Lambda(lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True))([branch, trunk])\n",
    "\n",
    "    # Add the bias\n",
    "    output = BiasLayer()(dot_product)\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.models.Model(inputs=[branch_input, trunk_input], outputs=output)\n",
    "\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "\n",
    "    return model\n",
    "\n",
    "class BiasLayer(tf.keras.layers.Layer):\n",
    "    def build(self, input_shape):\n",
    "        self.bias = self.add_weight(shape=(1,),\n",
    "                                    initializer=tf.keras.initializers.Zeros(),\n",
    "                                    trainable=True)\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.bias\n",
    "    \n",
    "def ODE_residual_calculator(t, u, u_t, model):\n",
    "    \"\"\"ODE residual calculation.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    t: temporal coordinate\n",
    "    u: input function evaluated at discrete temporal coordinates\n",
    "    u_t: input function evaluated at t\n",
    "    model: DeepONet model\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    ODE_residual: residual of the governing ODE\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(t)\n",
    "        s = model({\"forcing\": u, \"time\": t})\n",
    "\n",
    "    ds_dt = tape.gradient(s, t)\n",
    "    ODE_residual = ds_dt - u_t\n",
    "    return ODE_residual\n",
    "\n",
    "def train_step(X, X_init, IC_weight, ODE_weight, model):\n",
    "    \"\"\"Calculate gradients of the total loss with respect to network model parameters.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    X: training dataset for evaluating ODE residuals\n",
    "    X_init: training dataset for evaluating initial conditions\n",
    "    IC_weight: weight for initial condition loss\n",
    "    ODE_weight: weight for ODE loss\n",
    "    model: DeepONet model\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    ODE_loss: calculated ODE loss\n",
    "    IC_loss: calculated initial condition loss\n",
    "    total_loss: weighted sum of ODE loss and initial condition loss\n",
    "    gradients: gradients of the total loss with respect to network model parameters.\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(model.trainable_weights[-1].value)\n",
    "\n",
    "        # Initial condition prediction\n",
    "        y_pred_IC = model({\"forcing\": X_init[:, 1:-1], \"time\": X_init[:, :1]})\n",
    "\n",
    "        # Equation residual\n",
    "        ODE_residual = ODE_residual_calculator(t=X[:, :1], u=X[:, 1:-1], u_t=X[:, -1:], model=model)\n",
    "\n",
    "        # Calculate loss\n",
    "        MSE = keras.losses.MeanSquaredError()\n",
    "        IC_loss = tf.reduce_mean(MSE(0, y_pred_IC))\n",
    "        ODE_loss = tf.reduce_mean(tf.square(ODE_residual))\n",
    "\n",
    "        # Total loss\n",
    "        total_loss = IC_loss*IC_weight + ODE_loss*ODE_weight\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "\n",
    "    return ODE_loss, IC_loss, total_loss, gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data creation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_samples(length_scale, sample_num):\n",
    "    \"\"\"Create synthetic data for u(·)\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    length_scale: float, length scale for RNF kernel\n",
    "    sample_num: number of u(·) profiles to generate\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    u_sample: generated u(·) profiles\n",
    "    \"\"\"\n",
    "\n",
    "    # Define kernel with given length scale\n",
    "    kernel = RBF(length_scale)\n",
    "\n",
    "    # Create Gaussian process regressor\n",
    "    gp = GaussianProcessRegressor(kernel=kernel)\n",
    "\n",
    "    # Collocation point locations\n",
    "    X_sample = np.linspace(0, 1, 100).reshape(-1, 1) \n",
    "\n",
    "    # Create samples\n",
    "    u_sample = np.zeros((sample_num, 100))\n",
    "    for i in range(sample_num):\n",
    "        # sampling from the prior directly\n",
    "        n = np.random.randint(0, 10000)\n",
    "        u_sample[i, :] = gp.sample_y(X_sample, random_state=n).flatten()  \n",
    "\n",
    "    return u_sample\n",
    "\n",
    "def generate_dataset(N, length_scale, ODE_solve=False):\n",
    "    \"\"\"Generate dataset for Physics-informed DeepONet training.\n",
    "\n",
    "    Args:\n",
    "    ----\n",
    "    N: int, number of u(·) profiles\n",
    "    length_scale: float, length scale for RNF kernel\n",
    "    ODE_solve: boolean, indicate whether to compute the corresponding s(·)\n",
    "\n",
    "    Outputs:\n",
    "    --------\n",
    "    X: the dataset for t, u(·) profiles, and u(t)\n",
    "    y: the dataset for the corresponding ODE solution s(·)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create random fields\n",
    "    random_field = create_samples(length_scale, N)\n",
    "\n",
    "    # Compile dataset\n",
    "    X = np.zeros((N*100, 100+2))\n",
    "    y = np.zeros((N*100, 1))\n",
    "\n",
    "    for i in tqdm(range(N)):\n",
    "        u = np.tile(random_field[i, :], (100, 1))\n",
    "        t = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "        # u(·) evaluated at t\n",
    "        u_t = np.diag(u).reshape(-1, 1)\n",
    "\n",
    "        # Update overall matrix\n",
    "        X[i*100:(i+1)*100, :] = np.concatenate((t, u, u_t), axis=1)\n",
    "\n",
    "        # Solve ODE\n",
    "        if ODE_solve:\n",
    "            sol = solve_ivp(lambda var_t, var_s: np.interp(var_t, t.flatten(), random_field[i, :]), \n",
    "                            t_span=[0, 1], y0=[0], t_eval=t.flatten(), method='RK45')\n",
    "            y[i*100:(i+1)*100, :] = sol.y[0].reshape(-1, 1)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RBF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[190], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m N_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[1;32m      3\u001b[0m length_scale_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength_scale_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create validation dataset\u001b[39;00m\n\u001b[1;32m      7\u001b[0m N_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "Cell \u001b[0;32mIn[189], line 48\u001b[0m, in \u001b[0;36mgenerate_dataset\u001b[0;34m(N, length_scale, ODE_solve)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Generate dataset for Physics-informed DeepONet training.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03my: the dataset for the corresponding ODE solution s(·)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Create random fields\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m random_field \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Compile dataset\u001b[39;00m\n\u001b[1;32m     51\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((N\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m100\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m))\n",
      "Cell \u001b[0;32mIn[189], line 15\u001b[0m, in \u001b[0;36mcreate_samples\u001b[0;34m(length_scale, sample_num)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create synthetic data for u(·)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mu_sample: generated u(·) profiles\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Define kernel with given length scale\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m kernel \u001b[38;5;241m=\u001b[39m \u001b[43mRBF\u001b[49m(length_scale)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Create Gaussian process regressor\u001b[39;00m\n\u001b[1;32m     18\u001b[0m gp \u001b[38;5;241m=\u001b[39m GaussianProcessRegressor(kernel\u001b[38;5;241m=\u001b[39mkernel)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RBF' is not defined"
     ]
    }
   ],
   "source": [
    "# Create training dataset\n",
    "N_train = 2000\n",
    "length_scale_train = 0.4\n",
    "X_train, y_train = generate_dataset(N_train, length_scale_train)\n",
    "\n",
    "# Create validation dataset\n",
    "N_val = 100\n",
    "length_scale_test = 0.4\n",
    "X_val, y_val = generate_dataset(N_val, length_scale_test)\n",
    "\n",
    "# Create testing dataset\n",
    "N_test = 100\n",
    "length_scale_test = 0.4\n",
    "X_test, y_test = generate_dataset(N_test, length_scale_test, ODE_solve=True)\n",
    "\n",
    "# Determine batch size\n",
    "ini_batch_size = int(2000/100)\n",
    "col_batch_size = 2000\n",
    "\n",
    "# Create dataset object (initial conditions)\n",
    "X_train_ini = tf.convert_to_tensor(X_train[X_train[:, 0]==0], dtype=tf.float32)\n",
    "ini_ds = tf.data.Dataset.from_tensor_slices((X_train_ini))\n",
    "ini_ds = ini_ds.shuffle(5000).batch(ini_batch_size)\n",
    "\n",
    "# Create dataset object (collocation points)\n",
    "X_train = tf.convert_to_tensor(X_train, dtype=tf.float32)\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "train_ds = train_ds.shuffle(100000).batch(col_batch_size)\n",
    "\n",
    "# Scaling \n",
    "mean = {\n",
    "    'forcing': tf.convert_to_tensor(np.mean(X_train[:, 1:-1], axis=0), dtype=tf.float32),\n",
    "    'time': tf.convert_to_tensor(np.mean(X_train[:, :1], axis=0), dtype=tf.float32)\n",
    "}\n",
    "\n",
    "var = {\n",
    "    'forcing': tf.convert_to_tensor(np.var(X_train[:, 1:-1], axis=0), dtype=tf.float32),\n",
    "    'time': tf.convert_to_tensor(np.var(X_train[:, :1], axis=0), dtype=tf.float32)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saranyapujari/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "class LossTracking:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean_total_loss = keras.metrics.Mean()\n",
    "        self.mean_IC_loss = keras.metrics.Mean()\n",
    "        self.mean_ODE_loss = keras.metrics.Mean()\n",
    "        self.loss_history = defaultdict(list)\n",
    "\n",
    "    def update(self, total_loss, IC_loss, ODE_loss):\n",
    "        self.mean_total_loss(total_loss)\n",
    "        self.mean_IC_loss(IC_loss)\n",
    "        self.mean_ODE_loss(ODE_loss)\n",
    "\n",
    "    def reset(self):\n",
    "        self.mean_total_loss.reset_state()\n",
    "        self.mean_IC_loss.reset_state()\n",
    "        self.mean_ODE_loss.reset_state()\n",
    "\n",
    "    def print(self):\n",
    "        print(f\"IC={self.mean_IC_loss.result().numpy():.4e}, ODE={self.mean_ODE_loss.result().numpy():.4e}, total_loss={self.mean_total_loss.result().numpy():.4e}\")\n",
    "\n",
    "    def history(self):\n",
    "        self.loss_history['total_loss'].append(self.mean_total_loss.result().numpy())\n",
    "        self.loss_history['IC_loss'].append(self.mean_IC_loss.result().numpy())\n",
    "        self.loss_history['ODE_loss'].append(self.mean_ODE_loss.result().numpy())\n",
    "\n",
    "# Set up training configurations\n",
    "n_epochs = 300\n",
    "IC_weight= tf.constant(1.0, dtype=tf.float32)   \n",
    "ODE_weight= tf.constant(1.0, dtype=tf.float32)\n",
    "loss_tracker = LossTracking()\n",
    "val_loss_hist = []\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# Instantiate the PINN model\n",
    "PI_DeepONet= create_model(mean, var)\n",
    "PI_DeepONet.compile(optimizer=optimizer)\n",
    "\n",
    "# Configure callbacks\n",
    "_callbacks = [keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=30),\n",
    "             tf.keras.callbacks.ModelCheckpoint('NN_model.keras', monitor='val_loss', save_best_only=True)]\n",
    "callbacks = tf.keras.callbacks.CallbackList(\n",
    "                _callbacks, add_history=False, model=PI_DeepONet)\n",
    "\n",
    "# Start training process\n",
    "for epoch in range(1, n_epochs + 1):  \n",
    "    print(f\"Epoch {epoch}:\")\n",
    "\n",
    "    for X_init, X in zip(ini_ds, train_ds):\n",
    "\n",
    "        # Calculate gradients\n",
    "        ODE_loss, IC_loss, total_loss, gradients = train_step(X, X_init, \n",
    "                                                            IC_weight, ODE_weight,\n",
    "                                                            PI_DeepONet)\n",
    "        # Gradient descent\n",
    "        PI_DeepONet.optimizer.apply_gradients(zip(gradients, PI_DeepONet.trainable_variables))\n",
    "\n",
    "        # Loss tracking\n",
    "        loss_tracker.update(total_loss, IC_loss, ODE_loss)\n",
    "\n",
    "    # Loss summary\n",
    "    loss_tracker.history()\n",
    "    loss_tracker.print()\n",
    "    loss_tracker.reset()\n",
    "\n",
    "    ####### Validation\n",
    "    X_val_tensor = tf.convert_to_tensor(X_val, dtype=tf.float32)\n",
    "    val_res = ODE_residual_calculator(X_val_tensor[:, :1], X_val_tensor[:, 1:-1], X_val_tensor[:, -1:], PI_DeepONet)\n",
    "    val_ODE = tf.cast(tf.reduce_mean(tf.square(val_res)), tf.float32)\n",
    "\n",
    "    X_val_ini = X_val[X_val[:, 0]==0]\n",
    "    pred_ini_valid = PI_DeepONet.predict({\"forcing\": X_val_ini[:, 1:-1], \"time\": X_val_ini[:, :1]}, batch_size=12800)\n",
    "    MSE = tf.keras.losses.MeanSquaredError()\n",
    "    val_IC = tf.reduce_mean(MSE(0, pred_ini_valid))\n",
    "    print(f\"val_IC: {val_IC.numpy():.4e}, val_ODE: {val_ODE.numpy():.4e}, lr: {PI_DeepONet.optimizer.learning_rate.numpy():.2e}\")\n",
    "\n",
    "    # Callback at the end of epoch\n",
    "    callbacks.on_epoch_end(epoch, logs={'val_loss': val_IC+val_ODE})\n",
    "    val_loss_hist.append(val_IC+val_ODE)\n",
    "\n",
    "    # Re-shuffle dataset\n",
    "    ini_ds = tf.data.Dataset.from_tensor_slices((X_train_ini))\n",
    "    ini_ds = ini_ds.shuffle(5000).batch(ini_batch_size)\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train))\n",
    "    train_ds = train_ds.shuffle(100000).batch(col_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History\n",
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
    "ax[0].plot(range(n_epochs), loss_tracker.loss_history['IC_loss'])\n",
    "ax[1].plot(range(n_epochs), loss_tracker.loss_history['ODE_loss'])\n",
    "ax[2].plot(range(n_epochs), val_loss_hist)\n",
    "ax[0].set_title('IC Loss')\n",
    "ax[1].set_title('ODE Loss')\n",
    "ax[2].set_title('Val Loss')\n",
    "for axs in ax:\n",
    "    axs.set_yscale('log')\n",
    "plt.show()\n",
    "\n",
    "# Random Samples\n",
    "profiles = [1,2,3]\n",
    "t = X_test[0:100, 0:1]\n",
    "fig, ax = plt.subplots(2, 3, figsize=(12, 4))\n",
    "ax[0,0].plot(t, X_test[profiles[0] * 100:(profiles[0] + 1) * 100, 101])\n",
    "ax[0,0].set_xlabel(\"t\")\n",
    "ax[0,0].set_ylabel(\"u(t)\")\n",
    "ax[1,0].plot(t, y_test[profiles[0] * 100:(profiles[0] + 1) * 100, 0], label=\"Ground Truth\")\n",
    "ax[1,0].set_xlabel(\"t\")\n",
    "ax[1,0].set_ylabel(\"s(t)\")\n",
    "s_pred = PI_DeepONet.predict({\"forcing\": X_test[profiles[0] * 100:(profiles[0] + 1) * 100, 1:101], \"time\": t}, verbose=0)\n",
    "s_pred = s_pred.flatten()\n",
    "ax[1,0].plot(t, s_pred, label=\"Predicted\")\n",
    "ax[1,0].legend()\n",
    "\n",
    "ax[0,1].plot(t, X_test[profiles[1] * 100:(profiles[1] + 1) * 100, 101])\n",
    "ax[0,1].set_xlabel(\"t\")\n",
    "ax[0,1].set_ylabel(\"u(t)\")\n",
    "ax[1,1].plot(t, y_test[profiles[1] * 100:(profiles[1] + 1) * 100, 0], label=\"Ground Truth\")\n",
    "ax[1,1].set_xlabel(\"t\")\n",
    "ax[1,1].set_ylabel(\"s(t)\")\n",
    "s_pred = PI_DeepONet.predict({\"forcing\": X_test[profiles[1] * 100:(profiles[1] + 1) * 100, 1:101], \"time\": t}, verbose=0)\n",
    "s_pred = s_pred.flatten()\n",
    "ax[1,1].plot(t, s_pred, label=\"Predicted\")\n",
    "ax[1,1].legend()\n",
    "\n",
    "ax[0,2].plot(t, X_test[profiles[2] * 100:(profiles[2] + 1) * 100, 101])\n",
    "ax[0,2].set_xlabel(\"t\")\n",
    "ax[0,2].set_ylabel(\"u(t)\")\n",
    "ax[1,2].plot(t, y_test[profiles[2] * 100:(profiles[2] + 1) * 100, 0], label=\"Ground Truth\")\n",
    "ax[1,2].set_xlabel(\"t\")\n",
    "ax[1,2].set_ylabel(\"s(t)\")\n",
    "s_pred = PI_DeepONet.predict({\"forcing\": X_test[profiles[2] * 100:(profiles[2] + 1) * 100, 1:101], \"time\": t}, verbose=0)\n",
    "s_pred = s_pred.flatten()\n",
    "ax[1,2].plot(t, s_pred, label=\"Predicted\")\n",
    "ax[1,2].legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
